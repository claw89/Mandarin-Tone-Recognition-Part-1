{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandarin Tone Recognition Part 2\n",
    "===========\n",
    "This notebook is a continuation of Mandarin Tone Recognition Part 1\n",
    "\n",
    "A description of mandarin tones is available here: https://en.wikipedia.org/wiki/Standard_Chinese_phonology#Tones\n",
    "\n",
    "In part 1, we manually extracted tone lines from the spectrograms of mandarin tone recordings. Features were extracted from these lines, and these features were used to train a random forrest classifier. The resulting accuracy of approximately 75% was insufficient for practical applications. \n",
    "\n",
    "In this notebook, the method is altered in two ways: 1) In part 1, we used recordings of single character words only; part 2 uses words of any length. 2) Rather than the manual feature extraction used in part 1, this part uses the raw spectograms as the input for a convolutional neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Expansion\n",
    "\n",
    "In this section, we expand the wiktionary-based pronunciation dataset by including words with variable character length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "from pydub import AudioSegment\n",
    "from pydub import silence\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from scipy.stats import mode\n",
    "from tqdm import tqdm_notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the vocabularly lists for the HSK and TOCFL mandarin examinations to build the dataset. Overlapping entries are be removed.\n",
    "\n",
    "HSK: \n",
    "\n",
    "TOCFL: https://www.sc-top.org.tw/chinese/download.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8420 unique mandarin words extracted from the HSK and TOCFL vocab lists\n"
     ]
    }
   ],
   "source": [
    "files = ['hsk1.txt', \n",
    "         'hsk2.txt', \n",
    "         'hsk3.txt', \n",
    "         'hsk4.txt', \n",
    "         'hsk5.txt', \n",
    "         'hsk6.txt', \n",
    "         'tocflA.txt', \n",
    "         'tocflB.txt', \n",
    "         'tocflC.txt']\n",
    "lines = []\n",
    "for text_file in files:\n",
    "    with open(text_file, 'r', encoding='utf-8') as file:\n",
    "        lines += file.readlines()\n",
    "\n",
    "words = []\n",
    "for line in lines:\n",
    "    words.append(re.findall(r'[\\u4e00-\\u9fff]+', line)[0])\n",
    "\n",
    "words = list(set(words))\n",
    "print(\"{:d} unique mandarin words extracted from the HSK and TOCFL vocab lists\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions were adapted slightly from those used in part 1 to enable multi-character words. \n",
    "\n",
    "The function get_tones returns a list of the tone numbers for the supplied pinyin and hanzi. Some tone patterns result in changes to the pronunciation (e.g., two third tones are pronounced as a second tone followed by a third tone). The function tone_adjustments handles these changes for the majority of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby, cycle \n",
    "  \n",
    "def group_sequence(l): \n",
    "    \"\"\"\n",
    "    A helper function for tone adjustment\n",
    "    Takes a list of integers as its argument\n",
    "    Returns a list of tuples containing subsequences of l that increase by 1 for each index \n",
    "    e.g., [1, 2, 3, 1, 2, 2, 1, 1] returns [(1, 2, 3), (1, 2)]\n",
    "    \"\"\"\n",
    "    temp_list = cycle(l) \n",
    "    next(temp_list) \n",
    "    groups = groupby(l, key = lambda j: j + 1 == next(temp_list)) \n",
    "    for k, v in groups: \n",
    "        if k: \n",
    "            yield tuple(v) + (next((next(groups)[1])), ) \n",
    "\n",
    "def tone_adjustment(tones, hanzi):\n",
    "    \"\"\"\n",
    "    Adjusts a list of tones according to the rules for mandarin tone adjustment and return the corrected list of tones\n",
    "    (e.g., [3, 3] --> [2, 3])\n",
    "    \"\"\"\n",
    "    three_indexes = [i for i, v in enumerate(tones) if v ==3]\n",
    "    if len(three_indexes) > 0:\n",
    "        #Find the indexes of adjacent groups of third tones.\n",
    "        three_chains = list(group_sequence(three_indexes))\n",
    "        for three_chain in three_chains:\n",
    "            tones[min(three_chain):max(three_chain) + 1] = [2]*(max(three_chain) - min(three_chain)) + [3]\n",
    "    if \"不\" in hanzi and hanzi[-1] != \"不\":\n",
    "        if tones[hanzi.index(\"不\") + 1] == 4:\n",
    "            tones[hanzi.index(\"不\")] = 2\n",
    "    if \"一\" in hanzi and hanzi[-1] != \"一\":\n",
    "        following_char = hanzi[hanzi.index(\"一\")+1]\n",
    "        #There are more exceptions to the tone change rules for 一;\n",
    "        #they are rather more complex and appear unlikely to affect\n",
    "        #the multi-character words in this project. See following:\n",
    "        #https://resources.allsetlearning.com/chinese/pronunciation/Tone_change_rules\n",
    "        if following_char not in [\"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\"]:\n",
    "            if tones[hanzi.index(\"一\")+1] == 4:\n",
    "                tones[hanzi.index(\"一\")] = 2\n",
    "            else:\n",
    "                tones[hanzi.index(\"一\")] = 4\n",
    "    return tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tones(pinyin, hanzi):\n",
    "    \"\"\"\n",
    "    Returns a list of integers representing the tones for a pinyin word\n",
    "    \"\"\"\n",
    "    num_chars = len(hanzi)\n",
    "    tones = []\n",
    "    found_tones = re.findall(\"[āēīōūǖĀĒĪŌŪǕáéíóúǘÁÉÍÓÚǗǎěǐǒǔǚǍĚǏǑǓǙàèìòùǜÀÈÌÒÙǛ]\", pinyin)\n",
    "    if len(found_tones) == num_chars:\n",
    "        for sylable in found_tones:\n",
    "            if re.search(\"[āēīōūǖĀĒĪŌŪǕ]\", sylable):\n",
    "                tones.append(1)\n",
    "            elif re.search(\"[áéíóúǘÁÉÍÓÚǗ]\", sylable):\n",
    "                tones.append(2)\n",
    "            elif re.search(\"[ǎěǐǒǔǚǍĚǏǑǓǙ]\", sylable):\n",
    "                tones.append(3)\n",
    "            elif re.search(\"[àèìòùǜÀÈÌÒÙǛ]\", sylable):\n",
    "                tones.append(4)\n",
    "    elif len(found_tones) < num_chars:\n",
    "        for sylable in found_tones:\n",
    "            if re.search(\"[āēīōūǖĀĒĪŌŪǕ]\", sylable):\n",
    "                tones.append(1)\n",
    "            elif re.search(\"[áéíóúǘÁÉÍÓÚǗ]\", sylable):\n",
    "                tones.append(2)\n",
    "            elif re.search(\"[ǎěǐǒǔǚǍĚǏǑǓǙ]\", sylable):\n",
    "                tones.append(3)\n",
    "            elif re.search(\"[àèìòùǜÀÈÌÒÙǛ]\", sylable):\n",
    "                tones.append(4)\n",
    "        for i in range(num_chars - len(found_tones)):\n",
    "            tones.append(5)\n",
    "            \n",
    "    return tone_adjustment(tones, hanzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronuncitation(character, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns the standard pronunciation tone for the provided character\n",
    "    If no audio files are available for the character return False\n",
    "    Also saves the related audio file as ...\\word.ogg\n",
    "    \"\"\"\n",
    "    parser = WiktionaryParser()\n",
    "    try:\n",
    "        word = parser.fetch(character, 'chinese')\n",
    "    except AttributeError as error:\n",
    "        if verbose:\n",
    "            print(\"Attribute error for \", character, error)\n",
    "        return False\n",
    "    except KeyError as error:\n",
    "        if verbose:\n",
    "            print(\"Key error for \", character, error)\n",
    "        return False\n",
    "    if len(word) > 0:\n",
    "        if len(word[0]['pronunciations']['audio']) > 0:\n",
    "            if verbose:\n",
    "                print(word[0]['pronunciations']['text'][0])\n",
    "                print(\"tones: \", get_tones(word[0]['pronunciations']['text'][0].split(\":\")[1].split()[0], character))\n",
    "            tones = get_tones(word[0]['pronunciations']['text'][0].split(\":\")[1].split()[0], character)\n",
    "            url = 'https:' + word[0]['pronunciations']['audio'][0]\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            with open(r'word.ogg', 'wb') as file:\n",
    "                file.write(r.content)\n",
    "            return tones\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"No audio file available for this hanzi\")\n",
    "            return False\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"No audio file available for this hanzi\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_silence(data):\n",
    "    \"\"\"\n",
    "    Removes leading and trailing silences from the signal\n",
    "    Returns the signal with these silences removed.\n",
    "    \"\"\"\n",
    "    non_silent = [np.mean(np.abs(data[i:i+800])) for i in range(len(data)-799)]\n",
    "    non_silent = (non_silent > max(data)*0.02).tolist()\n",
    "    start = non_silent.index(True)-int(len(data)*0.005)\n",
    "    end = len(non_silent) - non_silent[::-1].index(True)+int(len(data)*0.005)\n",
    "    return data[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we loop over all the words in the list of unique HSK and TOCFL vocabulary to build the dataset. At each stage an entry is added to the dataframe df if the pronunciation is available. \n",
    "\n",
    "Since many of the words are composed of multiple characters, we must find a way to separate the sounds. Attempting to split on silence was unsuccessful, because distinguishable silence between characters is often not present. Therefore, we divide the audio data into equal chunks according to the number of characters expected. This imperfect approach is likely to lead to some undesired overlap (or leakage) between adjacent tone spectrograms. Consider improvements to this approach in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1ad0371c9f4348809c9a7183cc5e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3506), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"hanzi\": [], \n",
    "                   \"tone\": [], \n",
    "                   \"Sxx\": []})\n",
    "\n",
    "for word in tqdm_notebook(words):\n",
    "    num_chars = len(word)\n",
    "    tones = get_pronuncitation(word, verbose=False)\n",
    "    if tones:\n",
    "        try:\n",
    "            sound = AudioSegment.from_ogg(r'word.ogg')\n",
    "        except Exception as error:\n",
    "            pass\n",
    "        sound.export(r'word.wav', format=\"wav\")\n",
    "        #Open the wav file\n",
    "        fs, data = wavfile.read(r'word.wav')\n",
    "        if len(data.shape) > 1:\n",
    "            data = data.T[0]\n",
    "        #Trim leading and trailing silences\n",
    "        trimmed_data = trim_silence(data)\n",
    "        if len(trimmed_data) > 100:\n",
    "            for i in range(num_chars):\n",
    "                #Split the data into character chunks\n",
    "                chunk = trimmed_data[i*int(len(trimmed_data)/num_chars):(i+1)*int(len(trimmed_data)/num_chars)]\n",
    "                #Obtain spectograms\n",
    "                f, t, Sxx = signal.spectrogram(chunk, fs, nperseg=1024, nfft=1024*4)\n",
    "                #Trim the frequency domain\n",
    "                bound = len([freq for freq in f if freq < 1000])\n",
    "                Sxx = Sxx[:bound]\n",
    "                df = df.append(pd.DataFrame({\"hanzi\": [word], \n",
    "                                             \"tone\": [tones[i]], \n",
    "                                             \"Sxx\": [Sxx]}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting spectograms are of different sizes; we standardize them by adding zeros to create equally sized arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max([f for f, t in list(set(df.Sxx.apply(lambda x: x.shape).tolist()))])\n",
    "max_time = max([t for f, t in list(set(df.Sxx.apply(lambda x: x.shape).tolist()))])\n",
    "\n",
    "def pad_frequency(x):\n",
    "    if x.shape[0] < max_frequency:\n",
    "        return np.concatenate((x, np.zeros((max_frequency - x.shape[0], x.shape[1]))), axis=0)\n",
    "    else:\n",
    "        return x\n",
    "      \n",
    "def pad_time(x):\n",
    "    if x.shape[1] < max_time:\n",
    "        return np.concatenate((x, np.zeros((x.shape[0], max_time-x.shape[1]))), axis=1)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "df.Sxx = df.Sxx.apply(lambda x: pad_frequency(pad_time(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifier Training\n",
    "\n",
    "In this section we train a convolutional neural network (CNN) to classify the spectrograms into one of the 5 tones. CNNs have been successfully applied to the classification of handwritten digits using the MNIST dataset. Initially, we use the model recommended by keras for hand-written digit classification https://keras.io/examples/mnist_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 12\n",
    "\n",
    "img_rows, img_cols = max_frequency, max_time\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.Sxx.values\n",
    "X = np.array([x.tolist() for x in X.flatten()]).reshape(X.shape[0], img_rows, img_cols)\n",
    "y = df.tone.values\n",
    "\n",
    "#Normalize each image by dividing by the maximum value for that image.\n",
    "for i in range(X.shape[0]):\n",
    "    X[i] = X[i] / np.max(X[i])\n",
    "\n",
    "#Instead of tones 1-5, the classifier will predict 0-4. \n",
    "#Subtract 1 from all samples; 1 will be added to the \n",
    "#prediction result to yield the appropriate tone designation\n",
    "y = y - 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 92, 38, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 90, 36, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 45, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 45, 18, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 51840)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               6635648   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 6,655,109\n",
      "Trainable params: 6,655,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1475 samples, validate on 369 samples\n",
      "Epoch 1/12\n",
      " - 16s - loss: 1.1749 - acc: 0.5641 - val_loss: 0.7708 - val_acc: 0.7615\n",
      "Epoch 2/12\n",
      " - 14s - loss: 0.7399 - acc: 0.7620 - val_loss: 0.6348 - val_acc: 0.8130\n",
      "Epoch 3/12\n",
      " - 14s - loss: 0.5763 - acc: 0.8217 - val_loss: 0.5117 - val_acc: 0.8455\n",
      "Epoch 4/12\n",
      " - 14s - loss: 0.4284 - acc: 0.8678 - val_loss: 0.4755 - val_acc: 0.8645\n",
      "Epoch 5/12\n",
      " - 14s - loss: 0.3478 - acc: 0.8902 - val_loss: 0.4292 - val_acc: 0.8726\n",
      "Epoch 6/12\n",
      " - 15s - loss: 0.3064 - acc: 0.9044 - val_loss: 0.4173 - val_acc: 0.8889\n",
      "Epoch 7/12\n",
      " - 14s - loss: 0.2491 - acc: 0.9295 - val_loss: 0.4194 - val_acc: 0.8889\n",
      "Epoch 8/12\n",
      " - 14s - loss: 0.2064 - acc: 0.9376 - val_loss: 0.4488 - val_acc: 0.8862\n",
      "Epoch 9/12\n",
      " - 14s - loss: 0.1722 - acc: 0.9525 - val_loss: 0.4459 - val_acc: 0.8916\n",
      "Epoch 10/12\n",
      " - 14s - loss: 0.1500 - acc: 0.9553 - val_loss: 0.4703 - val_acc: 0.9024\n",
      "Epoch 11/12\n",
      " - 14s - loss: 0.1376 - acc: 0.9586 - val_loss: 0.5085 - val_acc: 0.8916\n",
      "Epoch 12/12\n",
      " - 15s - loss: 0.1084 - acc: 0.9634 - val_loss: 0.5638 - val_acc: 0.9024\n",
      "Test loss: 0.5637735302855329\n",
      "Test accuracy: 0.9024390247133043\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=12,\n",
    "          verbose=2,\n",
    "          validation_data=(X_test, y_test))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using a CNN and the addition of multi-character words yields a substantial improvement in prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User Input Classification\n",
    "\n",
    "Similarly to part 1, we would now like to take a recorded input from the user and classify the tones. Given that multi-character words were used for training, we should also be able to take a multi-character word input from the user. After triming the leading and trailing silences, the resulting data will be divided into equal chunks according to the number of characters expected. \n",
    "\n",
    "In some cases, the time dimension of the user input spectrogram may exceed that provided to the classifier during training. We can sample from the user input to reduce the time dimension, however this is not ideal and may lead to an improper classification. Consider adopting a recurrent neural network architecture to process sequence data of unknown length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tone(num_chars):\n",
    "    \"\"\"\n",
    "    Record an audio file from user; return the predicted tones for each character\n",
    "    \"\"\"\n",
    "    fs = 44000\n",
    "    seconds = 3 \n",
    "    print(\"Recording: Start\")\n",
    "    myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)\n",
    "    sd.wait()\n",
    "    print(\"Recording: Stop\")\n",
    "    write(r'rec.wav', fs, myrecording)\n",
    "    fs, data = wavfile.read(r'rec.wav')\n",
    "    if len(data.shape) > 1:\n",
    "        data = data.T[0]\n",
    "    #Trim leading and trailing silences\n",
    "    non_silent = [np.mean(np.abs(data[i:i+800])) for i in range(len(data)-799)]\n",
    "    non_silent = (non_silent > max(data)*0.02).tolist()\n",
    "    start = non_silent.index(True)-int(len(data)*0.005)\n",
    "    end = len(non_silent) - non_silent[::-1].index(True)+int(len(data)*0.005)\n",
    "    trimmed_data = data[start:end]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(num_chars):\n",
    "        chunk = trimmed_data[i*int(len(trimmed_data)/num_chars):(i+1)*int(len(trimmed_data)/num_chars)]\n",
    "        #Obtain spectograms\n",
    "        f, t, Sxx = signal.spectrogram(chunk, fs, nperseg=1024, nfft=1024*4)\n",
    "        #Trim the frequency domain\n",
    "        bound = len([freq for freq in f if freq < 1000])\n",
    "        Sxx = Sxx[:bound]\n",
    "        #Pad the time and frequency domains to match the classifier dimensions\n",
    "        Sxx = pad_frequency(pad_time(Sxx))\n",
    "        Sxx = Sxx / np.max(Sxx)\n",
    "        #TO DO: The size of the spectogram recorded from the user exceeds the \n",
    "        #size must be reduced somehow to fit the model\n",
    "        ##\n",
    "        ##\n",
    "        ##\n",
    "        ##\n",
    "        Sxx = Sxx.reshape(1, img_rows, img_cols, 1)\n",
    "        predictions.append(model.predict(Sxx))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording: Start\n",
      "Recording: Stop\n",
      "Sylable 0 prediction: 1\n",
      "Tone 1 probability: 99.950278%\n",
      "Tone 2 probability: 0.025686%\n",
      "Tone 3 probability: 0.012913%\n",
      "Tone 4 probability: 0.010186%\n",
      "Tone 5 probability: 0.000932%\n",
      "...\n",
      "Sylable 1 prediction: 2\n",
      "Tone 1 probability: 0.972835%\n",
      "Tone 2 probability: 61.734140%\n",
      "Tone 3 probability: 17.255640%\n",
      "Tone 4 probability: 16.305369%\n",
      "Tone 5 probability: 3.732009%\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_tone(num_chars=2)\n",
    "for i, sylable in enumerate(prediction):\n",
    "    print(\"Sylable {:d} prediction: {:d}\".format(i, np.argmax(sylable) + 1))\n",
    "    for j in range(5):\n",
    "        print(\"Tone {:d} probability: {:f}%\".format(j + 1, sylable[0][j]*100))\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "Remaining problems with this approach:\n",
    "\n",
    "1. Insufficient training data: although the ammount of training data was greatly increased by including multi-character words, more data are likely required to increase the accuracy of the classifier. For example, the MNIST dataset for which our classifier was designed, includes XXX samples for each category.\n",
    "2. Although the classifier achieved an accuracy of approximately 90% on the test data, testing with native mandarin speakers appears to yield considerably poorer results. This may be due to the lack of diversity in the training data (mostly male voice and Putonghua acccent); increasing the diversity of the data along with increasing the size of the dataset may improve the applicability of the classifier to a practical context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "d143180a8e3842ba979c6786519c4674": {
     "views": [
      {
       "cell_index": 68
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
